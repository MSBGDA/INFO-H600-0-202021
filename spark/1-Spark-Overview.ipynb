{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Core API by Examples\n",
    "\n",
    "This notebook illustrates how spark works by means of simple examples. The notebook is executed in python through pyspark (and jupyter).\n",
    "\n",
    "The PySpark Documentation is available at https://spark.apache.org/docs/latest/api/python/pyspark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General imports and starting Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is needed to start a Spark session from the notebook\n",
    "#You may adjust the memory used by the driver program based on your machine's settings\n",
    "import os \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can run in multiple modes, including:\n",
    "- **Local mode**: spark is run only on the same computer that runs this notebook. In this mode, spark can still exploit parallellism, by using all avaiable processor cores.\n",
    "- **Cluster mode**: spark is run using the resources made available by a resource manager at a cluster. The resource manager is responsible for allocating so-called *executor* instances (each with a number of CPU cores and an amount of memory) to spark. For this class, we will unfortunately not be able illustrate spark running in cluster mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Start Spark in LOCAL mode\n",
    "# -------------------------------\n",
    "\n",
    "#The following lines are just there to allow this cell to be re-executed multiple times:\n",
    "#if a spark session was already started, we stop it before starting a new one\n",
    "#(there can be only one spark context per jupyter notebook)\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"demoRDD\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: after creating the spark context in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have a working spark context, p;rint its configuration\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data/books` folder contains books (in txt format) downloaded from project gutenberg (http://www.gutenberg.org/cache/epub/). In that folder you will also find a shell script to download more books (`data/books/download_more_books.sh`), should you wish to experiment with a larger collection of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's list the files in data/books\n",
    "!ls data/books/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the contents of a file into an RDD. Note - when run on the cluster this load from HDFS (inside /user/$USER/)\n",
    "# if you really want to load from HDFS, you can also put the full HDFS url, e.g.\n",
    "# hdfs://public00:8020/user/<your_user_id_here>/data/books/pg20417.txt\n",
    "fileName = 'data/books/pg20417.txt'\n",
    "bookRDD = sc.textFile(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 5 elements of bookRDD\n",
    "bookRDD.take(5)\n",
    "# When loading a textfile, it is hence converted to an RDD where each line is one element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can operate on each element of an RDD by invoking the map() transformation.\n",
    "#\n",
    "# MAP : Return a new RDD by applying a function to each element of this RDD.\n",
    "# \n",
    "# Specifically, we split each line in multiples values by splitting on whitespace separators\n",
    "# The result is hence an RDD where each element is itself a list.\n",
    "tupleRDD = bookRDD.map(lambda line: line.split())\n",
    "tupleRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's turn the bookRDD into an RDD of words - each word becomes an element in the RDD\n",
    "# To do so, we use flatMap instead of map\n",
    "#\n",
    "# FLATMAP : Return a new RDD by first applying a function to all elements of this RDD, \n",
    "#           and then flattening the results.\n",
    "# \n",
    "wordsRDD = bookRDD.flatMap(lambda line: line.split())\n",
    "wordsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a wordCount by invoking the count() action\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can get the same result by using the aggregate() action. \n",
    "#\n",
    "# REDUCE : Reduces the elements of this RDD into a single value using the specified commutative and \n",
    "#          assocative binary operator. Currently reduces partitions locally.\n",
    "# Note: we first map the RDD into an RDD of integers\n",
    "wordsRDD.map(lambda x: 1).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: what happens if you reduce wordsRDD without mapping it first to an RDD of integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here: reduce wordsRDD without mapping it first to an RDD of integers. \n",
    "# Can you explain why this happens ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count how many unique words there are. The distinct() transformation removes duplicate elements.\n",
    "wordsRDD.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each unique word appears in wordsRDD by invoking the countByValue() transformation\n",
    "wordsRDD.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a wordcount on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBooksRDD = sc.textFile('data/books/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBooksRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordsRDD = allBooksRDD.flatMap(lambda line: line.split())\n",
    "allWordsRDD.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other transformations: sample, distinct, filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a sample of 10% of the words\n",
    "sample = allWordsRDD.sample(False, 0.1)\n",
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map to lower case, ignore duplicates\n",
    "lowerSample = sample.map(lambda w: w.lower()).distinct()\n",
    "lowerSample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter allows to get the subset of elements of an RDD that satisfy a given predicate\n",
    "startsWithARDD = allWordsRDD.filter(lambda w: w.startswith('a'))\n",
    "print(\"First three elements starting with \\'a\\'\", startsWithARDD.take(3))\n",
    "print(\"Number of elements starting with \\'a\\'\", startsWithARDD.count() )\n",
    "print(\"Number of distinct elements starting with \\'a\\'\", startsWithARDD.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: the code above counts all words that start with lowercase letter 'a'. Using the same filter predicate (i.e., `lambda w: w.startswith('a')`, calculate the number of words that start with either 'a' or 'A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default aggregations on RDD of ints/RDD of doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various easy aggregation function are pre-defined on RDDs containing only integers\n",
    "intsRDD = sc.parallelize(range(1,10))\n",
    "intsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsRDD.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsRDD.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsRDD.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsRDD.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION : Build the union of a list of RDDs; the result will have duplicates\n",
    "\n",
    "one = sc.parallelize(range(1,10))\n",
    "two = sc.parallelize(range(9,21))\n",
    "print(\"One : \",one.collect())\n",
    "print(\"Two : \",two.collect())\n",
    "print(\"One union Two : \",one.union(two).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERSECTION : Return the intersection of this RDD and another one. \n",
    "# The output will not contain any duplicate elements, even if the input RDDs did.\n",
    "\n",
    "one = sc.parallelize(range(1,10))\n",
    "two = sc.parallelize(range(5,15))\n",
    "print(\"One : \",one.collect())\n",
    "print(\"Two : \",two.collect())\n",
    "print(\"One intersection Two : \",one.intersection(two).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract computes set difference (without duplicates)\n",
    "one = sc.parallelize(range(1,10))\n",
    "two = sc.parallelize(range(5,15))\n",
    "print(\"One : \",one.collect())\n",
    "print(\"Two : \",two.collect())\n",
    "print(\"One difference Two : \",one.subtract(two).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An RDD is partitioned transparently\n",
    "\n",
    "# Using the glom() transformation we can inspect how the elements of an RDD are partitioned.\n",
    "# glom() coalesces all elements within each partition into a list. \n",
    "\n",
    "# Using the repartition() function we can transform an RDD into an RDD  \n",
    "# that has exactly numPartitions partitions. We Can increase or decrease the level of\n",
    "# parallelism in this RDD. Internally, this uses a shuffle to redistribute data. \n",
    "\n",
    "# We ask the colletion to have 5 partitions\n",
    "rdd = sc.parallelize(range(50), 5).map(str)\n",
    "glomed = rdd.glom()\n",
    "print(\"Initial RDD : \",rdd.collect())\n",
    "print(\"Glomed RDD : \",glomed.collect())\n",
    "print(\"# of partitions : \",rdd.getNumPartitions())\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "rdd = sc.parallelize(range(50), 4)        # We specify 4 partitions\n",
    "print(\"# of partitions before \",rdd.getNumPartitions())\n",
    "print(\"Data in partitions : \",rdd.collect())\n",
    "glomed = rdd.glom()\n",
    "print(\"Glomed RDD : \",glomed.collect())\n",
    "\n",
    "print(\"-----------------------------\")\n",
    "repartitionedRDD = rdd.repartition(10)\n",
    "print(\"# of partitions after \",repartitionedRDD.getNumPartitions())\n",
    "print(\"Data in partitions : \",repartitionedRDD.collect())\n",
    "glomed = repartitionedRDD.glom()\n",
    "print(\"Glomed RDD : \",glomed.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair RDDs are RDDs that contain pairs of the form (key, value)\n",
    "elems = [ ('a', [1,2,3,4]), ('b', [2,3,4]), ('c', [5,9,11,12]), ('a', [100,101])]\n",
    "pairRDD = sc.parallelize(elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair RDDs have some special operations defined on them\n",
    "\n",
    "# mapValues: pass each value in the key-value pair RDD through a map function without changing the keys\n",
    "# this also retains the original RDD’s partitioning.\n",
    "print(\"Mapping values to a string: \", pairRDD.mapValues(lambda value: str(value)).collect())\n",
    "print(\"Mapping values to take only first element:\", pairRDD.mapValues(lambda value: value[0]).collect())\n",
    "print(\"Mapping values to sum of elements:\", pairRDD.mapValues(lambda value: sum(value)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also group all values that have the same key. Each key then has a value that is a collection\n",
    "pairRDD.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can actually do the usual kind of things on these collections\n",
    "pairRDD.groupByKey().mapValues(lambda value: len(value)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following converts the value from a ResultIterable to a normal list\n",
    "pairRDD.groupByKey().mapValues(lambda value: [x for x in value]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducebykey is a parallel reduce operation, that reduces per key the set of all \n",
    "# associated values into a single value\n",
    "pairRDD.reduceByKey(lambda x,y: x+y).collect()   #Note: + on lists is list concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by key is similar, but it allows us more flexibility on how to combine\n",
    "# In the following example, we start with an initial aggregate value 0. Then, \n",
    "# for each (key, val) pair in the RDD, val is first aggregated using lambda x, y: x+sum(y)\n",
    "# Note that in this function, x is the aggregate value so far and y is the key-value (in our case it is a list)\n",
    "# the new aggregate is obtained by adding the old aggregate value to the sum of the list. \n",
    "# Finally, multiple aggregate values for the same key are combined into a single one using lambda x,y: x+y,\n",
    "# i.e., just by adding the per-pair aggregate values.\n",
    "pairRDD.aggregateByKey(0, lambda x, y: x+ sum(y), lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION!\n",
    "# Be careful about using pairRDD methods on non-pair RDDs. The result is not always what you expect.\n",
    "elems = [ [1,2,3,4], [2,3,4,5]]\n",
    "rdd = sc.parallelize(elems)\n",
    "print(rdd.collect())\n",
    "print(rdd.mapValues(lambda x: x).collect())\n",
    "\n",
    "# Note that hence each list is viewed as a pair, where the first element is the key and the second\n",
    "# element is the value, the rest is discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair RDDs can also be sorted, either by key, or by some computed sort key\n",
    "elems = [ ('a', [1,2,3,4]), ('b', [2,3,4]), ('c', [5,9,11,12]), ('a', [100,101])]\n",
    "pairRDD = sc.parallelize(elems)\n",
    "print(\"sorted by key:\", pairRDD.sortByKey().collect())\n",
    "print(\"sorted descendingly by length of the value:\", pairRDD.sortBy(lambda x: len(x[1]), False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal RDDs can be transformed into pair RDDs by the groupBy transformation\n",
    "# GroupBy takes a function f that returns, for each element in the old RDD, its key in the new rdd\n",
    "# All elements with the same key are grouped together in a value\n",
    "elems = ['abcd', 'abracadabra', 'hello', 'hi', 'bottom', 'top']\n",
    "rdd = sc.parallelize(elems)\n",
    "print(\"grouped by first letter:\\n\", str(rdd.groupBy(lambda x: x[0]).collect()))\n",
    "print(\"grouped by first letter, values mapped to lists:\\n\", rdd.groupBy(lambda x: x[0]).mapValues(lambda x: [y for y in x]).collect())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is **vital** that you stop your spark instance after you are done working with it. If you do not do this, the resources acquired by your spark instance (i.e., the number of cores and memory reserved for it) will be kept indefinetely, and *are hence would not available for others if when using spark in cluster mode!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
