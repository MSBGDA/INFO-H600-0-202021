{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## 1. Sensor data exercises\n",
    "In the file “data/sensors/sensor-sample.txt” you will find on each line, multiple fields of information, let’s call them : Date(Date), Time(Time), RoomId(Integer)-SensorId(Integer), Value1(float), Value2(float)\n",
    "Using this file, use spark to compute the following queries :\n",
    "\n",
    "1. Count the number of entries for each day.\n",
    "2. Count the number of measures for each pair of RoomId-SensorId.\n",
    "3. Compute the average of Value1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "# Starting spark and creating a new session\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "config = SparkConf().setAppName('sensor')\\\n",
    "                    .setMaster('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1606425401953'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.port', '41433'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'sensor'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', '0f940969f428')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sensor-sample.txt'\n",
    "sensorRDD = sc.textFile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-03-31 03:38:16.508 1-0 122.153 2.03397',\n",
       " '2017-03-31 03:38:15.967 1-1 -3.91901 2.09397',\n",
       " '2017-03-31 03:38:16.577 1-2 11.04 2.07397',\n",
       " '2017-02-28 00:59:16.359 1-0 19.9884 2.74964',\n",
       " '2017-02-28 00:59:16.803 1-1 37.0933 2.76964',\n",
       " '2017-02-28 00:59:16.526 1-2 45.08 2.77964',\n",
       " '2017-02-28 01:03:17.244 1-0 19.3024 2.72742',\n",
       " '2017-02-28 01:03:16.972 1-1 38.4629 2.74742',\n",
       " '2017-02-28 01:03:16.914 1-2 45.08 2.77742',\n",
       " '2017-02-28 01:06:17.010 1-0 19.1652 2.70742']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing first few elements of the RDD\n",
    "sensorRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Count the number of entries for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-03-31',\n",
       " '2017-03-31',\n",
       " '2017-03-31',\n",
       " '2017-02-28',\n",
       " '2017-02-28',\n",
       " '2017-02-28',\n",
       " '2017-02-28',\n",
       " '2017-02-28',\n",
       " '2017-02-28',\n",
       " '2017-02-28']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting each line and extracting only the date column\n",
    "day_RDD = sensorRDD.map(lambda line: line.split()[0])\n",
    "day_RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'2017-03-31': 3393,\n",
       "             '2017-02-28': 62103,\n",
       "             '2017-03-01': 33423,\n",
       "             '2017-03-02': 32403,\n",
       "             '2017-03-03': 29727,\n",
       "             '2017-03-04': 30225,\n",
       "             '2017-03-05': 26019,\n",
       "             '2017-03-06': 24315,\n",
       "             '2017-03-07': 26625,\n",
       "             '2017-03-08': 29343,\n",
       "             '2017-03-09': 27288,\n",
       "             '2017-03-21': 19410,\n",
       "             '2017-03-22': 10989,\n",
       "             '2017-03-10': 12483,\n",
       "             '2017-03-23': 24213,\n",
       "             '2017-03-24': 13467,\n",
       "             '2017-03-11': 19059,\n",
       "             '2017-03-12': 25089,\n",
       "             '2017-03-25': 12225,\n",
       "             '2017-03-13': 24783,\n",
       "             '2017-03-26': 13587,\n",
       "             '2017-03-14': 23418,\n",
       "             '2017-03-27': 14544,\n",
       "             '2017-03-15': 11901,\n",
       "             '2017-03-28': 22338,\n",
       "             '2017-03-29': 12120,\n",
       "             '2017-03-16': 13869,\n",
       "             '2017-03-17': 26922,\n",
       "             '2017-03-30': 5814,\n",
       "             '2017-03-18': 17427,\n",
       "             '2017-03-19': 21999,\n",
       "             '2017-03-20': 21942,\n",
       "             '2017-04-01': 537})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number how many times each date appears in day_RDD\n",
    "day_RDD.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "from operator import add\n",
    "date_count = day_RDD.map(lambda day: (day, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the results\n",
    "from operator import itemgetter\n",
    "sorted_items = sorted(date_count.collect(), key=itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-02-28: 62103\n",
      "2017-03-01: 33423\n",
      "2017-03-02: 32403\n",
      "2017-03-04: 30225\n",
      "2017-03-03: 29727\n",
      "2017-03-08: 29343\n",
      "2017-03-09: 27288\n",
      "2017-03-17: 26922\n",
      "2017-03-07: 26625\n",
      "2017-03-05: 26019\n",
      "2017-03-12: 25089\n",
      "2017-03-13: 24783\n",
      "2017-03-06: 24315\n",
      "2017-03-23: 24213\n",
      "2017-03-14: 23418\n",
      "2017-03-28: 22338\n",
      "2017-03-19: 21999\n",
      "2017-03-20: 21942\n",
      "2017-03-21: 19410\n",
      "2017-03-11: 19059\n",
      "2017-03-18: 17427\n",
      "2017-03-27: 14544\n",
      "2017-03-16: 13869\n",
      "2017-03-26: 13587\n",
      "2017-03-24: 13467\n",
      "2017-03-10: 12483\n",
      "2017-03-25: 12225\n",
      "2017-03-29: 12120\n",
      "2017-03-15: 11901\n",
      "2017-03-22: 10989\n",
      "2017-03-30: 5814\n",
      "2017-03-31: 3393\n",
      "2017-04-01: 537\n"
     ]
    }
   ],
   "source": [
    "# printing out the results\n",
    "max_len = max([len(day) for day, count in sorted_items])\n",
    "for day, count in sorted_items:\n",
    "    print(f'{day:>{max_len}}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Two: Count the number of measures for each pair of RoomId-SensorId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-03-31 03:38:16.508 1-0 122.153 2.03397',\n",
       " '2017-03-31 03:38:15.967 1-1 -3.91901 2.09397',\n",
       " '2017-03-31 03:38:16.577 1-2 11.04 2.07397',\n",
       " '2017-02-28 00:59:16.359 1-0 19.9884 2.74964',\n",
       " '2017-02-28 00:59:16.803 1-1 37.0933 2.76964']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing first few elements of the RDD\n",
    "sensorRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-0', '1-1', '1-2', '1-0', '1-1', '1-2', '1-0', '1-1', '1-2', '1-0']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting each line and extracting only the roomID-sensorID column\n",
    "id_RDD = sensorRDD.map(lambda line: line.split()[2])\n",
    "id_RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'1-0': 43047,\n",
       "             '1-1': 43047,\n",
       "             '1-2': 43047,\n",
       "             '2-0': 46915,\n",
       "             '2-1': 46915,\n",
       "             '2-2': 46915,\n",
       "             '3-0': 46634,\n",
       "             '3-1': 46634,\n",
       "             '3-2': 46634,\n",
       "             '4-0': 43793,\n",
       "             '4-1': 43793,\n",
       "             '4-2': 43793,\n",
       "             '5-0': 35,\n",
       "             '5-1': 35,\n",
       "             '5-2': 35,\n",
       "             '6-0': 35666,\n",
       "             '6-1': 35666,\n",
       "             '6-2': 35666,\n",
       "             '7-0': 14910,\n",
       "             '7-1': 14910,\n",
       "             '7-2': 14910})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number how many times each date appears in day_RDD\n",
    "id_RDD.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compute the average of Value1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['122.153',\n",
       " '-3.91901',\n",
       " '11.04',\n",
       " '19.9884',\n",
       " '37.0933',\n",
       " '45.08',\n",
       " '19.3024',\n",
       " '38.4629',\n",
       " '45.08',\n",
       " '19.1652']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting each line and extracting only the roomID-sensorID column\n",
    "value1_RDD = sensorRDD.map(lambda line: line.split()[3])\n",
    "value1_RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.8069927576456"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing the average\n",
    "value1_RDD.map(lambda x: float(x)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping sonsor spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Movielens movie data exercises\n",
    "\n",
    "Movielens (https://movielens.org/) is a website that provides non-commercial, personalised movie recommendations. GroupLens Research has collected and made available rating data sets from the MovieLens web site for the purpose of research into making recommendation services. In this exercise, we will use one of these datasets (the movielens latest dataset, http://files.grouplens.org/datasets/movielens/ml-latest-small.zip) and compute some basic queries on it.\n",
    "The dataset has already been downloaded and is available at data/movielens/movies.csv, data/movielens/ratings.csv, data/movielens/tags.csv, data/movielens/links.csv\n",
    "\n",
    "1. Inspect the dataset's [README file](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html), in particular the section titled \"Content and Use of Files\" to learn the structure of these three files.\n",
    "2. Compute all pairs (`movieid`, `rat`) where `movieid` is a movie id (as found in ratings.csv) and `rat` is the average rating of that movie id. (Hint: use aggregateByKey to compute first the sum of all ratings as well as the number of ratings per key).\n",
    "2. Compute all pairs (`title`, `rat`) where `title` is a full movie title (as found in the movies.csv file), and `rat` is the average rating of that movie (computed over all possible ratings for that movie, as found in the ratings.csv file)\n",
    "3. [_Extra_] Compute all pairs (`title`, `tag`) where `title` is a full movie title that has an average rating of at least 3.5, and `tag` is a tag for that movie (as found in the tags.csv file)\n",
    "\n",
    "Extra: if you want to experiment with larger datasets, download the 10m dataset (http://files.grouplens.org/datasets/movielens/ml-10m.zip, 250 Mb uncompressed) and re-do the exercises above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "# Starting spark and creating a new session\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "config = SparkConf().setAppName('movie')\\\n",
    "                    .setMaster('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Rating per movie ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '4.0'), ('3', '4.0'), ('6', '4.0'), ('47', '5.0'), ('50', '5.0')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the rating file as RDD with just the movieid and rating value column.\n",
    "movie_RDD = sc.textFile('ratings.csv').map(lambda line: (line.split(',')[1], float(line.split(',')[2])))\n",
    "movie_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_sum_RDD = movie_RDD.groupByKey().mapValues(sum)\n",
    "movie_num_RDD = movie_RDD.groupByKey().mapValues(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3.9209302325581397),\n",
       " ('50', 4.237745098039215),\n",
       " ('70', 3.5090909090909093),\n",
       " ('110', 4.031645569620253),\n",
       " ('157', 2.8636363636363638)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rating_RDD = rating_sum_RDD.join(movie_num_RDD).mapValues(lambda x: (x[0]/x[1]))\n",
    "avg_rating_RDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average rating per movie title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'Toy Story (1995)'),\n",
       " ('2', 'Jumanji (1995)'),\n",
       " ('3', 'Grumpier Old Men (1995)'),\n",
       " ('4', 'Waiting to Exhale (1995)'),\n",
       " ('5', 'Father of the Bride Part II (1995)')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_RDD2 = sc.textFile('movies.csv').map(lambda line: (line.split(',')[0], (line.split(',')[1])))\n",
    "movie_RDD2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Waiting to Exhale (1995)', 2.357142857142857),\n",
       " ('GoldenEye (1995)', 3.496212121212121),\n",
       " ('Dracula: Dead and Loving It (1995)', 2.4210526315789473),\n",
       " ('Casino (1995)', 3.926829268292683),\n",
       " ('Money Train (1995)', 2.5)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = movie_RDD2.join(avg_rating_RDD).map(lambda x: x[1])\n",
    "result.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show movie titles and tags only for movies with atleast 3.5 rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('60756', 'funny'),\n",
       " ('60756', 'Highly quotable'),\n",
       " ('60756', 'will ferrell'),\n",
       " ('89774', 'Boxing story'),\n",
       " ('89774', 'MMA')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_RDD3 = sc.textFile('tags.csv').map(lambda line: (line.split(',')[1], (line.split(',')[2])))\n",
    "movie_RDD3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3.9209302325581397),\n",
       " ('50', 4.237745098039215),\n",
       " ('70', 3.5090909090909093),\n",
       " ('110', 4.031645569620253),\n",
       " ('163', 3.5606060606060606)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movies with alteast 3.5 rating\n",
    "movie_RDD4 = avg_rating_RDD.filter(lambda x: x[1]>=3.5)\n",
    "movie_RDD4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Casino (1995)', (3.926829268292683, 'Mafia')),\n",
       " ('Babe (1995)', (3.65234375, 'Animal movie')),\n",
       " ('Babe (1995)', (3.65234375, 'pigs')),\n",
       " ('Babe (1995)',\n",
       "  (3.65234375, 'villain nonexistent or not needed for good story')),\n",
       " ('\"Cry', (4.25, 'In Netflix queue'))]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = movie_RDD2.join(movie_RDD4.join(movie_RDD3)).map(lambda x: x[1])\n",
    "result2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Github log data exercises\n",
    "Github makes activity logs publicly available at https://www.githubarchive.org/. One such log file, which contains activity data for 2015-03-01 between 0h-1h at night, has been downloaded and is available at `data/github/2015-03-01-0.json.gz`. This (compressed) file contains multiple JSON objects, one per line. Here is a sample line of this file, neatly formatted:\n",
    "\n",
    "`{ \"id\": \"2614896652\",\n",
    "    \"type\": \"CreateEvent\",\n",
    "    \"actor\": {\n",
    "        \"id\": 739622,\n",
    "        \"login\": \"treydock\",\n",
    "        \"gravatar_id\": \"\",\n",
    "        \"url\": \"https://api.githb.com/users/treydock\",\n",
    "        \"avatar_url\": \"https://avatars.githubusercontent.com/u/739622?\"\n",
    "    },\n",
    "    \"repo\": {\n",
    "        \"id\": 23934080,\n",
    "        \"name\": \"Early-Modern-OCR/emop-dashboard\",\n",
    "    \"url\": \"https://api.github.com/repos/Early-Modern-OCR/emop-dashboard\"\n",
    "    },\n",
    "    \"payload\": {\n",
    "        \"ref\": \"development\",\n",
    "        \"ref_type\": \"branch\",\n",
    "        \"master-branch\": \"master\",\n",
    "        \"description\": \"\",\n",
    "        \"pusher_type\": \"user\",\n",
    "    },\n",
    "    \"public\": true,\n",
    "    \"created_at\": \"2015-03-01T00:00:00Z\",\n",
    "    \"org\": {\n",
    "        \"id\": 10965476,\n",
    "        \"login\": \"Early-Modern-OCR\",\n",
    "        \"gravatar_id\": \"\",\n",
    "        \"url\": \"https://api.github.com/orgs/Early-Modern-OCR\",\n",
    "        \"avatar_url\": \"https://avatars.githubusercontent.com/u/10965476?\"\n",
    "    }\n",
    "}`\n",
    "\n",
    "This log entry has `CreateEvent` type and its `payload.ref_type` is `branch` . So someone named \"treydock\" (`actor.login`) created a repository branch called \"development\" (`payload.ref`) in the first second of March 1, 2015 (`created_at`) .\n",
    "\n",
    "1. Load the textfile into an RDD (note: spark can read gzipped files directly!). Convert this RDD (which consists of string elements) to an RDD where each element is a JSON object (hint: use the `json.loads` function from the `json` module to convert a string into a JSON object).\n",
    "\n",
    "2. Filter this RDD of JSON objects to retain only those objects that represent push activities (where `type` equals `PushEvent`)\n",
    "\n",
    "3. Count the number of push events.\n",
    "\n",
    "4. Compute the number of push events, grouped per `actor.login`. \n",
    "\n",
    "5. Retrieve the results of (4) in sorted order, where logins with higher number of pushes come first. Retrieve the 10 first such results (which contain the highest number of pushes)\n",
    "\n",
    "6. You are representing a company and need to retrieving the number of pushes for every employee in the company. The file `data/github/employees.txt` contains a list of all employee login names at your company.\n",
    "\n",
    "Extra: if you want to experiment with larger datasets, download more log data from the github archive website and re-do the exercises above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
