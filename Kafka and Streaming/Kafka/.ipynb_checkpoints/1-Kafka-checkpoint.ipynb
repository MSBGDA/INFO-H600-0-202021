{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary note:\n",
    "This notebook assumes that you are running it on the cluster. You can run it on a local machine, but take into account that you need to modify the addresses of zookeeper and the brokers\n",
    "\n",
    "\n",
    "# Kafka\n",
    "\n",
    "**Kafka** is a distributed publish-subscribe message system focused on high throughput. All messages ingested by Kafka are persisted on disk and are also replicated within a given cluster to garantee fault-tolerance and thus prevent data loss. However, Kafka relies on another service, **ZooKeeper** which offers Kafka the synchronisation information it needs to run properly in a distributed way.\n",
    "\n",
    "The benefits of Kafka are thus : \n",
    "\n",
    "- *Reliability* as it is distributed, partitioned, replicated and offers fault tolerance.\n",
    "- *Scalability* as we can either increase or descrease the actual size of a Kafka cluster on demand and without downtime to best fit the actual load.\n",
    "- *Durabiliy* as all messages are persisted on disk.\n",
    "- *Performance* as it is designed for High Throughput.\n",
    "\n",
    "What can Kafka be used for ?\n",
    "\n",
    "- *Log aggregation* even though other systems may even be more appropriate for that purpose now.\n",
    "- *Metrics* as it can ingest vasts amounts of data, collecting metrics from multiple servers was a strong point for monitoring system even though, again, other systems do exist now for such scenarios.\n",
    "- *Stream Processing* Now we are talking. Framework like Storm and Spark Streaming can take advantage of Kafka to process information in a streaming fashion thus offering us different means of making data analysis. Indeed, in MapReduce, you only do Batch-Processing, you cannot handle continuous influx of information without having to invest lots of time in automating your submissions were in Spark Streaming for instance, you can make an application run permanently and analysing data as it arrives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka concepts\n",
    "\n",
    "Using Kafka, you have to be familiar with the concepts it uses such as *topics*, *brokers*, *producers* and *consumers*.\n",
    "\n",
    "![alt text](https://www.tutorialspoint.com/apache_kafka/images/fundamentals.jpg \"Kafka Architecture with a replication factor of 3\")\n",
    "\n",
    "- **Topic** A stream of messages belonging to a particular category is called a topic. Data is stored in topics.\n",
    "- **Partition** Topics are split into partitions. For each topic, Kafka keeps a minimum of one partition. Each such partition contains messages in an immutable ordered sequence. **Partition offset ** Each partitioned message has a unique sequence id called as offset.\n",
    "- **Replicas** Replicas are nothing but backups of a partition. They are used to prevent data loss.\n",
    "- **Broker** Brokers are worker processes that are responsible for maintaining the published data (accespting data form produces, serving it to consumers). Each broker may have zero or more partitions per topic. \n",
    "- **Kafka cluster** The set of all Kafka brokers. A Kafka cluster can be expanded without downtime. These clusters are used to manage the persistence and replication of message data.\n",
    "- **Producers** are Publishers of messages\n",
    "- **Consumers** are Consumers of messages\n",
    "- **Leader** Node responsible for all reads and write for a given partition, thus, each partition has one broker acting as leader.\n",
    "- **Follower** Node which just follows the **Leader** instructions. A Follower may become a Leader if the node attributed the role of Leader fails at some point. In practise, a Follower acts just as a consumer, consuming data from its Leader to maintain its own data store.\n",
    "- **ZooKeeper** ZooKeeper is used for managing and coordinating Kafka broker. ZooKeeper service is mainly used to notify producer and consumer about the presence of any new broker in the Kafka system or failure of the broker in the Kafka system. As per the notification received by the Zookeeper regarding presence or failure of the broker then pro-ducer and consumer takes decision and starts coordinating their task with some other broker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Kafka from Command Line\n",
    "\n",
    "## Starting Kafka\n",
    "\n",
    "On your computer or on the VM, you need to start zookeeper and kafka as explained in the instruction file. Since Jupyter Notebooks do not support background processes, you have to run them from a terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $KAFKA_HOME/bin/zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties \n",
    "# $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Listing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kafka-topics command can be used to list available topics, create new ones, and delete existing ones\n",
    "# we have to specify the host+port where zookeeper is running (since zookeeper keeps the metadata of all topics)\n",
    "# note: we pipe stderr to /dev/null, otherwise you'll get lots of INFO log messages \n",
    "# (when kafka is installed on your machine this is normally not necessary)\n",
    "!kafka-topics.sh --list --zookeeper localhost:2181 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n",
      "Created topic bigdata.test1.\n"
     ]
    }
   ],
   "source": [
    "# Let's create a new topic.\n",
    "#\n",
    "# \n",
    "# Here we create a topic <username>.test1, which is split into 5 partitions, \n",
    "# which each partition replicated three times\n",
    "#\n",
    "# If you want to increase the replication factor, you have to start more than one instance of kafka brokers as\n",
    "# explained in the instruction file.\n",
    "!kafka-topics.sh --create --zookeeper localhost:2181 \\\n",
    "    --replication-factor 1 --partitions 5 --topic \"$USER.test1\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigdata.test1\r\n"
     ]
    }
   ],
   "source": [
    "!kafka-topics.sh --list --zookeeper localhost:2181 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic bigdata.test1 is marked for deletion.\r\n",
      "Note: This will have no impact if delete.topic.enable is not set to true.\r\n"
     ]
    }
   ],
   "source": [
    "!kafka-topics.sh --delete --zookeeper localhost:2181 --topic \"$USER.test1\" 2>/dev/null\n",
    "!kafka-topics.sh --list --zookeeper localhost:2181 2>/dev/null "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a producer - With file input\n",
    "\n",
    "**Attention:** the following examples assumes that you have a folder \"INFOH515/books\" in your current working directory.\n",
    "We created this folder and put example books in that folder in the 1st lab session. Only execute the following command if you deleted this folder+files in the meantime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./INFOH515\n",
    "!mkdir ./INFOH515/books\n",
    "\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20417/pg20417.txt -O ./INFOH515/books/pg20417.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20418/pg20418.txt -O ./INFOH515/books/pg20418.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20419/pg20419.txt -O ./INFOH515/books/pg20419.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20420/pg20420.txt -O ./INFOH515/books/pg20420.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20421/pg20421.txt -O ./INFOH515/books/pg20421.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20422/pg20422.txt -O ./INFOH515/books/pg20422.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20423/pg20423.txt -O ./INFOH515/books/pg20423.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20424/pg20424.txt -O ./INFOH515/books/pg20424.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20425/pg20425.txt -O ./INFOH515/books/pg20425.txt\n",
    "!wget --quiet http://www.gutenberg.org/cache/epub/20426/pg20426.txt -O ./INFOH515/books/pg20426.txt\n",
    "!echo \"Books downloaded in ./INFOH515/books\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First re-create the topic\n",
    "!kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 5 --topic \"$USER.test1\" 2>/dev/null\n",
    "\n",
    "# Now publish messages on this topic\n",
    "# kafka-console-producer is a shell command that reads from stdin and published every line read\n",
    "# on the specified topic as a separate message\n",
    "# to use it, we need to specify the address of at least one kafka broker in our cluster \n",
    "# (in our case: localhost at port 9092, see the $KAFKA_HOME/config/server.properties file)\n",
    "!kafka-console-producer.sh --broker-list localhost:9092 --topic \"$USER.test1\" < ./INFOH515/books/pg20417.txt 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a consumer - From the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's read all message published on the topic, starting from the beginning\n",
    "# kafka-console-consumer reads from the topic, and prints every message on std out\n",
    "# again, we need to specify the address of at least one kafka broker in our cluster\n",
    "# (in this notebook we are using only one broker)\n",
    "# the --from-beginning flag means that we start readding from the beginning of the stream, \n",
    "# instead of reading from the end (waiting for new items to arrive)\n",
    "#\n",
    "# NOTE: this command will keep waiting for new messages to appear on the kafka topic and will \n",
    "#       hence never terminate. \n",
    "#       You'll need to cancel it (click on  \"Kernel\" the menu bar, and then on \"interrupt\") \n",
    "#       before going to the next one\n",
    "#\n",
    "# NOTE: If you look at the text that is output, you will see that some sentences may appear in an incorrect order\n",
    "#       This is due to the fact that we partitioned the topic into 5 partitions and we read from all 5 of \n",
    "#       these partititions in parallel. While the order is preserved inside a paritition, it is not preserved\n",
    "#       across partitions\n",
    "!kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic \"$USER.test1\" --from-beginning 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kafka-topics.sh --describe --zookeeper localhost:2181 --topic \"$USER.test1\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming a topic from a given offset\n",
    "We can also start consuming a topic at a specific offset. If you run the following command, you'll see that the first 10 lines are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interrupt the execution of this cell if it takes too long!\n",
    "!kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic $USER.test1 --partition 0 --offset 10 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing through Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kafka-topics.sh --delete --zookeeper localhost:2181 --topic \"$USER.test1\" 2>/dev/null\n",
    "!kafka-topics.sh --list --zookeeper localhost:2181 2>/dev/null \n",
    "!kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 5 --topic \"$USER.test1\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pwd\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "topic = pwd.getpwuid( os.getuid() )[ 0 ] + \".test1\"\n",
    "producer = KafkaProducer(bootstrap_servers=[\"localhost:9092\"])\n",
    "\n",
    "producer.send(topic, b'Hello world !!!')\n",
    "producer.send(topic, b'Ok, go check the consumer !')\n",
    "producer.send(topic, b'Bye!')\n",
    "print(\"Messsage sent to topic : \"+topic)\n",
    "\n",
    "# you can also send messages as key/value pairs. \n",
    "# It is ensured that all messages with the same key (if it is not None) will end up in the same partition.\n",
    "# The key needs to be of type bytes or bytearray \n",
    "producer.send(topic, key=b\"some_key_1\", value=b'Hello world with a key!')\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming through Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : this script keeps running waiting for new messages. Interrupt the kernel to abort it!\n",
    "import os\n",
    "import pwd\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "topic = pwd.getpwuid( os.getuid() )[ 0 ] + \".test1\"\n",
    "\n",
    "# if you do not specify auto_offset_reset, you resume consuming where you left off last time\n",
    "consumer = KafkaConsumer(topic, bootstrap_servers=[\"localhost:9092\"], auto_offset_reset='earliest')\n",
    "\n",
    "print(\"Waiting for data to consume from topic %s...\" % topic)\n",
    "\n",
    "for message in consumer:\n",
    "    print(message.topic, message.partition, message.key, message.value)\n",
    "    \n",
    "# If you prefer to consume JSON messages that could be more practical\n",
    "# KafkaConsumer(value_deserializer=lambda m: json.loads(m.decode('ascii')))      \n",
    "\n",
    "# If you desire to consume a specific pattern of topic\n",
    "# consumer = KafkaConsumer()\n",
    "# consumer.subscribe(pattern='^awesome.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:The Kafka server is not secured with per user access rules. So, to avoid any collision or interferences between you, we advise you to name your topics using systematically your NETID as prefix !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new Kafka topic named `<userid>.books` where `<userid>` is your netid\n",
    "2. Write a Python script that will read all books downloaded previously and store them in the kafka topic `<userid>.books` where `userid` is your netid. Make sure that all lines of the same book are put in the same topic partition. (Hint: use the fact that messages with the same key are put in the same partition.)\n",
    "3. Create a new Kafka topic names `<userid>.book-count` where <userid> is your netid\n",
    "4. Write a python script that will read from the Kafka `<userid>.books` topic and count the number of lines in each book. Whenever a count is updated, it publishes a message `(bookname, current_count)` to `<userid>.book-count`\n",
    "\n",
    "*Important*: for the last python script, pass the options `auto_offset_reset='earliest'` and `enable_auto_commit=False` when creating the KafkaConsumer object, this ensures that you will be able to re-read the stream if you re-execute the python stream multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
