{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises on Spark Streaming\n",
    "\n",
    "The objective in this set of exercises is to get comfortable with streaming-style computations using Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frist, a gentle reminder of Spark Streaming\n",
    "\n",
    "\n",
    "![alt text](https://spark.apache.org/docs/latest/img/streaming-arch.png \"\")\n",
    "\n",
    "Spark Streaming is a Spark library that allows streaming computations. \n",
    "\n",
    "In essence, Spark Streaming receives input data streams from any type of services (i.e: Kafka, Flume, HDFS, Kinesis, Twitter,...) and divides them into *mini-batches*. Those mini batches are then processed by Spark to build a final stream of results in batches. \n",
    "\n",
    "![alt text](https://spark.apache.org/docs/latest/img/streaming-flow.png \"\")\n",
    "\n",
    "A continuous sequence of mini-batches is called a DStream. Each mini-batch in this DStream gets represented as an RDD and Spark Streaming provide a high level API that manipulate DStreams. \n",
    "\n",
    "![alt text](https://spark.apache.org/docs/latest/img/streaming-dstream.png) \n",
    "\n",
    "Any operation that is applied on a DStream translates to operations on the underlying RDDs. If we consider a simple example where the stream of data is a stream of lines of words (i.e: simple sentences for instance), a flatMap operation is applied on each RDD in the lines to generate as output the words DStreams containing a list of the words present in the processed sentence.\n",
    "\n",
    "![alt text](https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Streaming programming guide is available at https://spark.apache.org/docs/latest/streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the combination of the following elements, how we execute spark streaming jobs is very different from how we experimented with Spark itself in the last exercise session.\n",
    "\n",
    "1. Streaming computations are computations that never finish (they continuously wait for new data to arrive).\n",
    "2. We will need to run multiple computations in parallel (1 computation to generate data, 1 to consume data).\n",
    "3. Jupyter does not allow cells to be executed in parallel.\n",
    "\n",
    "**Therefore**, while we will run the streaming computations inside of this notebook, you will need to launch a new notebook to launch, in parallel,  the process that generates the data. (The exact instructions may be found below.)\n",
    "\n",
    "### How to launch spark applications\n",
    "\n",
    "In the last lab session, we create the Spark Context (necessary to start spark transformations and actions) *inside* of the jupyter notebook itself (reusing the spark context across many cells). Because we will often need to stop the streaming computation (which closes the context), in this session, we will put all the computation inside of a single python script, and launch this script through the command line. There are different ways to do this. \n",
    "- If you invoke it by `python <name-of-your-script.py>` the script will be run in local mode (i.e. only on the machine on which you are invoking `<name-of-your-script.py>`). This will work only if pyspark is correctly added to your PYTHONPATH variable.\n",
    "- Alternatively, you can invoke it by `spark-submit <name-of-your-script.py>`. In this case the script will also be run in local mode by default. This assumes that the spark-submit command (found in the `bin` subfolder of your spark distribution) is in your path. **This is the preferred way, used below.**\n",
    "- You can also pass arguments to spark-submit. For example `spark-submit --master yarn <name-of-your-script` deploys the script to YARN, which will schedule it on a cluster. This will not be shown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple Spark streaming example: counting inside a mini-batch\n",
    "\n",
    "As already mentioned above, Spark Streaming can receive its input from different types of services, inlcuding Kafka, Twitter, Kinesis, ... . The simplest kind of streaming source, however, is the *file system*. In particular, when you set up Spark Streaming to receive data from a specific folder (which can be on your local filesystem, but could also be on HDFS), then it will watch this folder for new files to occur. Every new file will be treated as one mini-batch in the DStream. It is important to note that files that already existed in the watched folder when spark streaming starts will **not be processed**, only new files will be processed!\n",
    "\n",
    "In this exercise session, we will first use the file system as a source of streaming data. The last section of this notebook has an example of connecting to Kafka.\n",
    "\n",
    "We next describe the data that we will be using. The `data` subfolder contains a file `data/orders.txt` that contains some historal data of buy and sell orders on a stock exchange. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-03-22 20:25:28,1,80,EPE,710,51.00,B\\n',\n",
       " '2016-03-22 20:25:28,2,70,NFLX,158,8.00,B\\n',\n",
       " '2016-03-22 20:25:28,3,53,VALE,284,5.00,B\\n',\n",
       " '2016-03-22 20:25:28,4,14,SRPT,183,34.00,B\\n',\n",
       " '2016-03-22 20:25:28,5,62,BP,241,36.00,S\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the first 5 lines of orders.txt\n",
    "import headtail\n",
    "headtail.head('data/orders.txt', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line has the following fields.\n",
    "* Order timestamp—Format yyyy-mm-dd hh:MM:ss\n",
    "* Order ID —Serially incrementing integer\n",
    "* Client ID —Integer randomly picked from the range 1 to 100\n",
    "* Stock symbol—Randomly picked from a list of 80 stock symbols\n",
    "* Number of stocks to be bought or sold—Random number from 1 to 1,000\n",
    "* Price at which to buy or sell—Random number from 1 to 100\n",
    "* Character B or S —Whether the event is an order to buy or sell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of ` data/orders.txt` is split into multiple files in the subfolder `data/split`. For example, `data/split/ordersaa.ordtmp` contains the first 1000 lines of `data/orders.txt`; `ordersab.ordtmp` contains the next 1000, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-03-22 20:25:28,1,80,EPE,710,51.00,B\\n',\n",
       " '2016-03-22 20:25:28,2,70,NFLX,158,8.00,B\\n',\n",
       " '2016-03-22 20:25:28,3,53,VALE,284,5.00,B\\n',\n",
       " '2016-03-22 20:25:28,4,14,SRPT,183,34.00,B\\n',\n",
       " '2016-03-22 20:25:28,5,62,BP,241,36.00,S\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import headtail\n",
    "headtail.head('data/split/ordersaa.ordtmp', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-03-22 20:25:28,1001,73,AAPL,798,42.00,S\\n',\n",
       " '2016-03-22 20:25:28,1002,99,NQ,303,50.00,S\\n',\n",
       " '2016-03-22 20:25:28,1003,29,PBR,988,67.00,B\\n',\n",
       " '2016-03-22 20:25:28,1004,40,Z,327,27.00,B\\n',\n",
       " '2016-03-22 20:25:28,1005,96,PBR,587,46.00,B\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import headtail\n",
    "headtail.head('data/split/ordersab.ordtmp', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python script `scripts/simulateStreamingInput.py` can be used to simulate new data arriving in a streaming fashion. Concretely, it copies the files from `data/split` to  the folder `stream-IN` one by one, with a delay of 3 seconds in-between two files. If we start Spark streaming to monitor the `stream-IN` folder for new files, then the net effect is that every 3 seconds, 1000 lines of stock trade date is made available to Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1.1.** The file `1-countPerBatch.py` creates a Spark Streaming job that monitors the folder `stream-IN`for new files. For each mini-batch (which contains the contents of these new files), it will parse each text line in mini-batch into a python dictionary. Next, it computes the total number of lines that contain a *Buy* \n",
    "order(last column = 'B') and the total number of lines that contain a *Sell* order (last column = 'F'). Finally, the first 10 lines of each mini-batch are printed on the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1-countPerBatch.py\n"
     ]
    }
   ],
   "source": [
    "%%file 1-countPerBatch.py\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "STREAM_IN = 'stream-IN'\n",
    "STREAM_OUT = 'stream-OUT'\n",
    "\n",
    "# We first delete all files from the STREAM_IN folder\n",
    "# before starting spark streaming.\n",
    "# This way, all files produced by scripts/simulateStreamingInput are new\n",
    "print(\"Deleting existing files in %s ...\" % STREAM_IN)\n",
    "p = Path('.') / STREAM_IN\n",
    "for f in p.glob(\"*.ordtmp\"):\n",
    "    os.remove(f)\n",
    "print(\"... done\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext()      #Default spark context, arguments (e.g, cluster, memory) can be passed through spark-submit\n",
    "sc.setLogLevel(\"WARN\")   #Make sure warnings and errors observed by spark are printed, but not INFO messages.\n",
    "\n",
    "# Uncomment the following if you want to see exactly with which arguments the spark context has been created.\n",
    "#print(\"----------------- SC CONF------------------\")\n",
    "#print(sc._conf.getAll())\n",
    "#print(\"------------------------------------------\")\n",
    "\n",
    "ssc = StreamingContext(sc, 5)  #generate a mini-batch every 5 seconds\n",
    "filestream = ssc.textFileStream(STREAM_IN) #monitor new files in folder stream-IN\n",
    "\n",
    "def parseOrder(line):\n",
    "  '''parses a single line in the orders file into a dictionary'''\n",
    "  s = line.split(\",\")\n",
    "  try:\n",
    "      if s[6] != \"B\" and s[6] != \"S\":\n",
    "        raise Exception('Wrong format')\n",
    "      return [{\"time\": datetime.strptime(s[0], \"%Y-%m-%d %H:%M:%S\"),\n",
    "               \"orderId\": int(s[1]), \n",
    "               \"clientId\": int(s[2]),\n",
    "               \"symbol\": s[3], \n",
    "               \"amount\": int(s[4]), \n",
    "               \"price\":  float(s[5]), \n",
    "               \"buy\": s[6] == \"B\"}]\n",
    "  except Exception as err:\n",
    "      print(\"Wrong line format (%s): %s\" % (line,err))\n",
    "      return [] #ignore this line since it threw an error while parsing\n",
    "\n",
    "# Convert the input DStream (where each RDD contains lines) into a\n",
    "# DStream of python dictionaries (where each RDD contains dictionaries) \n",
    "# flatMap applies parseOrder on each line in each RDD in\n",
    "# the DStream, where results are flattened\n",
    "orders = filestream.flatMap(parseOrder)\n",
    "\n",
    "from operator import add\n",
    "\n",
    "# Calculate total number of buy/sell orders (buy -> key = True, sell -> key = False)\n",
    "# map applies its argument function on each RDD in the DStream\n",
    "# reduceByKey applies reduceBykey on each RDD in the DStream\n",
    "numPerType = orders.map(lambda o: (o['buy'], 1)).reduceByKey(add)\n",
    "\n",
    "# Print the first 10 lines of each RDD computed in the DStream to stdou\n",
    "# This is usefull for debugging purposes only\n",
    "numPerType.pprint()\n",
    "\n",
    "# -----ALTERNATIVE TO PPRINT----\n",
    "# If instead you want to save each computed RDD to a file, uncomment the following\n",
    "# This creates a new folder for each RDD computed; inside the folder 1 file for\n",
    "#  each partition in the rdd is created. To make this easy to inspect, we\n",
    "# repartition the RDD into 1 single partition (but this is not required).\n",
    "# ------------------------------\n",
    "# numPerType.repartition(1).saveAsTextFiles(STREAM_OUT, \"txt\")\n",
    "\n",
    "# Now start consuming input and wait forever (or until you press CTRL+C)\n",
    "# When run from inside jupyter, click on menu Kernel -> Interrupt to press CTRL=C\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2** Do the following.\n",
    "\n",
    "1. Inspect the contents of the file `1-countPerBatch.py`. See if you understand what is being done\n",
    "2. Execute this python script in **local** mode by means of `spark-submit`. In parallel (i.e., in a separate notebook/shell/command line), execute `python scripts/simulateStreamingInput.py` to start copying data to the `stream-IN` folder.\n",
    "3. You can terminate the Spark Streaming and `simulateStreamingInput` jobs by interrupting your notebook (which the same as pressing CTRL+C on a command line).\n",
    "4. Modify `1-countPerBatch.py` and uncomment the line that saves every RDD in the DStream to stream-OUT. Re-execute to see what happens\n",
    "\n",
    "**note:** We run in local mode because we are reading from the local filesystem. If we want to execute it in a distributed fashion, we would need other scripts for generating the data (which construct new files in HDFS instead of on the local filesystem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/11/18 15:19:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleting existing files in stream-IN ...\n",
      "... done\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/11/18 15:19:29 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/11/18 15:19:29 INFO SparkContext: Submitted application: 1-countPerBatch.py\n",
      "20/11/18 15:19:29 INFO SecurityManager: Changing view acls to: bigdata\n",
      "20/11/18 15:19:29 INFO SecurityManager: Changing modify acls to: bigdata\n",
      "20/11/18 15:19:29 INFO SecurityManager: Changing view acls groups to: \n",
      "20/11/18 15:19:29 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/11/18 15:19:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bigdata); groups with view permissions: Set(); users  with modify permissions: Set(bigdata); groups with modify permissions: Set()\n",
      "20/11/18 15:19:30 INFO Utils: Successfully started service 'sparkDriver' on port 39223.\n",
      "20/11/18 15:19:30 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/11/18 15:19:30 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/11/18 15:19:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/11/18 15:19:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/11/18 15:19:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bc7d6f5b-13b0-4ff9-a654-051e1657fe53\n",
      "20/11/18 15:19:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/11/18 15:19:30 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/11/18 15:19:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/11/18 15:19:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://bigdata-vm:4040\n",
      "20/11/18 15:19:31 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/11/18 15:19:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44725.\n",
      "20/11/18 15:19:32 INFO NettyBlockTransferService: Server created on bigdata-vm:44725\n",
      "20/11/18 15:19:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/11/18 15:19:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bigdata-vm, 44725, None)\n",
      "20/11/18 15:19:32 INFO BlockManagerMasterEndpoint: Registering block manager bigdata-vm:44725 with 366.3 MB RAM, BlockManagerId(driver, bigdata-vm, 44725, None)\n",
      "20/11/18 15:19:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bigdata-vm, 44725, None)\n",
      "20/11/18 15:19:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bigdata-vm, 44725, None)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/Desktop/exercises_tests/3 - Kafka and Streaming/Spark Streaming/1-countPerBatch.py\", line 77, in <module>\n",
      "    ssc.awaitTermination()\n",
      "  File \"/home/bigdata/spark/python/lib/pyspark.zip/pyspark/streaming/context.py\", line 192, in awaitTermination\n",
      "  File \"/home/bigdata/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
      "  File \"/home/bigdata/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "  File \"/home/bigdata/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/bigdata/spark/python/lib/pyspark.zip/pyspark/context.py\", line 270, in signal_handler\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Run the 1-countPerBatch in local mode (i.e., on the current machine)\n",
    "# (Note: Assumes that spark-submit executable is in your path.)\n",
    "# Execute 'scripts/simulateStreamingInput.py' in a separate notebook to generate\n",
    "# the input to this spark streaming file.\n",
    "!spark-submit --master \"local[*]\" 1-countPerBatch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Some comments**:\n",
    "\n",
    "* Just like you need a SparkContext object to construct RDDs, you need a StreamingContext to construct DStreams. StreamingContext are created from an existing SparkContext.\n",
    "* Only 1 StreamingContext can be executing per JVM, i.e., 1 per spark streaming job\n",
    "* You can stop a StreamingContext `ssc` by calling `ssc.stop()`. This, however will also close the SparkContext that was used to create it. Call `ssc.stop(False)` to avoid closing the SparkContext (which can then be used to construct a new StreamingContext)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 1.3** Copy `1-countPerBatch.py` into a file `1.3-countAndVolumePerBatch.py` and modify the latter to output, for each mini-batch, the following pairs:\n",
    "\n",
    "```\n",
    "('BUY', total number of buy orders in this minibatch RDD)\n",
    "('SELL', total number of sell orders in this minibatch RDD)\n",
    "('BUYVOL', total volume bought in this minibatch RDD)\n",
    "('SELLVOL', total volume sold in this minibatch RDD)\n",
    "```\n",
    "Here, the *volume* of an order is the order's amount times the order's price.\n",
    "\n",
    "(Hint: create two dstreams, one for the counts and one for the volumes, and union them  with the `union` method of dstreams).\n",
    "\n",
    "Be sure to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file 1.3-countAndVolumePerBatch.py\n",
    "\n",
    "# Put your solution here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --master \"local[*]\" 1.3-countAndVolumePerBatch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.4** Copy `1-countPerBatch.py` into a file `1.4-countAndVolumePerBatch.py` and modify the latter to output, for each mini-batch, the following pairs:\n",
    "```\n",
    "('BUY': total number of buy orders in this minibatch)\n",
    "('SELL': total number of sell orders in this minibatch)\n",
    "('<userid>': total volume traded (bought or sold) by this user-id in this mini-batch)\n",
    "```\n",
    "Where the last pair is repeated for every `<userid>` present in the current minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file 1.4-countAndVolumePerBatch.py\n",
    "\n",
    "# Put your solution here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --master \"local[*]\" 1.4-countAndVolumePerBatch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aggregating data across mini-batches\n",
    "\n",
    "Often we need to compute aggregates of data that spans multiple mini-batches. The file `2-totalVolumePerClient.py` will output, for each mini-batch, the following pairs:\n",
    "\n",
    "```\n",
    "('BUY': total number of buy orders in this minibatch RDD)\n",
    "('SELL': total number of sell orders in this minibatch RDD)\n",
    "('<userid>': total volume traded by this user-id across all mini-batches, present and past)\n",
    "```\n",
    "Where the last pair is repeated for every `<userid>` ever encountered and the total volume includes both buys and sells.\n",
    "\n",
    "It works by using the `updateStateBykey` function of pair DStreams, which allows remember a state (per key) across minibatches. Concretely, updateStateByKey takes as argument function that gets two inputs: the set of new values for the key (in this minibatch) and the old state (which is `None` if the key hasn't been seen before). It needs to output the new state to be maintained. This also becomes part of the output RDD.\n",
    "\n",
    "In our example, the state is just the current volume bought and sold, i.e., an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 2-totalVolumePerClient.py\n"
     ]
    }
   ],
   "source": [
    "%%file 2-totalVolumePerClient.py\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "STREAM_IN = 'stream-IN'\n",
    "STREAM_OUT = 'stream-OUT'\n",
    "\n",
    "# We first delete all files from the STREAM_IN folder\n",
    "# before starting spark streaming.\n",
    "# This way, all files are new\n",
    "print(\"Deleting existing files in %s ...\" % STREAM_IN)\n",
    "p = Path('.') / STREAM_IN\n",
    "for f in p.glob(\"*.ordtmp\"):\n",
    "    os.remove(f)\n",
    "print(\"... done\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext()      #Default spark context, arguments (e.g, cluster, memory) can be passed through spark-submit\n",
    "sc.setLogLevel(\"WARN\")   #Make sure warnings and errors observed by spark are printed.\n",
    "\n",
    "ssc = StreamingContext(sc, 5)  #generate a mini-batch every 5 seconds\n",
    "filestream = ssc.textFileStream(STREAM_IN) #monitor new files in folder stream-IN\n",
    "\n",
    "def parseOrder(line):\n",
    "  '''parses a single line in the orders file'''\n",
    "  s = line.split(\",\")\n",
    "  try:\n",
    "      if s[6] != \"B\" and s[6] != \"S\":\n",
    "        raise Exception('Wrong format')\n",
    "      return [{\"time\": datetime.strptime(s[0], \"%Y-%m-%d %H:%M:%S\"),\n",
    "               \"orderId\": int(s[1]), \n",
    "               \"clientId\": int(s[2]),\n",
    "               \"symbol\": s[3], \n",
    "               \"amount\": int(s[4]), \n",
    "               \"price\":  float(s[5]), \n",
    "               \"buy\": s[6] == \"B\"}]\n",
    "  except Exception as err:\n",
    "      print(\"Wrong line format (%s): %s\" % (line,err))\n",
    "      return []\n",
    "\n",
    "orders = filestream.flatMap(parseOrder)\n",
    "\n",
    "from operator import add\n",
    "\n",
    "# Calculate total number of buy/sell orders (buy -> key = True, sell -> key = False)\n",
    "numPerType = orders.map(lambda o: (\"BUY\", 1) if o['buy'] else (\"SELL\", 1)).reduceByKey(add)\n",
    "\n",
    "volumePerClient = orders.map(lambda o: (o['clientId'], o['amount'] * o['price']))\n",
    "volumeState = volumePerClient.updateStateByKey(lambda vals, totalOpt: sum(vals) + totalOpt if totalOpt != None else sum(vals))\n",
    "\n",
    "finalStream = numPerType.union(volumeState)\n",
    "finalStream.pprint(50)\n",
    "\n",
    "#finalStream.repartition(1).saveAsTextFiles(STREAM_OUT, \"txt\")\n",
    "\n",
    "# updateStateByKey requires checkpointing; set the spark checkpoint\n",
    "# folder to the subfolder of the current folder named \"checkpoint\"\n",
    "sc.setCheckpointDir(\"checkpoint\")\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**\n",
    "1. Inspect the contents of the file `2-totalVolumePerclient.py`. See if you understand what is being done\n",
    "2. Execucte this python script. In parallel (i.e., in a separate shell/command line), execute `scripts/simulateStreamingInput.py` to start copying data to the `stream-IN` folder.\n",
    "3. You can terminate the Spark Streaming and `simulateStreamingInput` jobs by pressing control+C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --master \"local[*]\" 2-totalVolumePerClient.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2** Copy `2-totalVolumePerClient.py` into a file `2.2-top5Clients.py` and \n",
    "modify the latter to output, for each mini-batch, the user ids of the top 5 clients (i.e., the 5 clients that have the largest buy/sell volume over all orders seen so far).\n",
    "\n",
    "*Hint*: to calculate the top-5 elements of an RDD you can first sort the RDD (using `sortBy`) and then then take the first 5 elements (first `zipWithIndex` to associate the index to each element, then filter only those elements whose index is less than 5). Note, however, that a DStream is a sequence of RDDs, not a single RDD. So, you need to do this transformation on each rdd in the DStream, which you can do by means of the DStream's `transform()` method (which takes as argument a function that transforms the RDD).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file 2.2-top5Clients.py\n",
    "\n",
    "# Your solution here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --master \"local[*]\" 2.2-top5Clients.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time-limited aggregates using windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using windowing operations, we can time-limited aggregates. \n",
    "\n",
    "**Example 3.1** An example is given in `3-salesPerMinutes.py`, which computes the total number of orders seen in the last minute, with a refresh of this total every 15 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3-salesPerMinutes.py\n"
     ]
    }
   ],
   "source": [
    "%%file 3-salesPerMinutes.py\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "STREAM_IN = 'stream-IN'\n",
    "STREAM_OUT = 'stream-OUT'\n",
    "\n",
    "# We first delete all files from the STREAM_IN folder\n",
    "# before starting spark streaming.\n",
    "# This way, all files are new\n",
    "print(\"Deleting existing files in %s ...\" % STREAM_IN)\n",
    "p = Path('.') / STREAM_IN\n",
    "for f in p.glob(\"*.ordtmp\"):\n",
    "  os.remove(f)\n",
    "print(\"... done\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext()      #Default spark context, arguments (e.g, cluster, memory) can be passed through spark-submit\n",
    "sc.setLogLevel(\"WARN\")   #Make sure warnings and errors observed by spark are printed.\n",
    "\n",
    "ssc = StreamingContext(sc, 5)  #generate a mini-batch every 5 seconds\n",
    "filestream = ssc.textFileStream(STREAM_IN) #monitor new files in folder stream-IN\n",
    "\n",
    "def parseOrder(line):\n",
    "  '''parses a single line in the orders file'''\n",
    "  s = line.split(\",\")\n",
    "  try:\n",
    "      if s[6] != \"B\" and s[6] != \"S\":\n",
    "        raise Exception('Wrong format')\n",
    "      return [{\"time\": datetime.strptime(s[0], \"%Y-%m-%d %H:%M:%S\"),\n",
    "               \"orderId\": int(s[1]), \n",
    "               \"clientId\": int(s[2]),\n",
    "               \"symbol\": s[3], \n",
    "               \"amount\": int(s[4]), \n",
    "               \"price\":  float(s[5]), \n",
    "               \"buy\": s[6] == \"B\"}]\n",
    "  except Exception as err:\n",
    "      print(\"Wrong line format (%s): %s\" % (line,err))\n",
    "      return []\n",
    "\n",
    "from operator import add\n",
    "orders = filestream.flatMap(parseOrder)\n",
    "ordersPerMinute = orders.map(lambda o: 1).window(60, 15) # window length = 60 sec, slide = 15 sec\n",
    "orderCountPerMinute = ordersPerMinute.reduce(add)\n",
    "orderCountPerMinute.pprint()\n",
    "\n",
    "# windows operations requires checkpointing; set the spark checkpoint\n",
    "# folder to the subfolder of the current folder named \"checkpoint\"\n",
    "sc.setCheckpointDir(\"checkpoint\")\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2**  \n",
    "1. Inspect the contents of the file `3-salesPerMinute.py`. See if you understand what is being done\n",
    "2. Execucte this python script. In parallel (i.e., in a separate jupyter notebook/shell/command line), execute `scripts/simulateStreamingInput.py` to start copying data to the `stream-IN` folder. The totals reported should increased during the first minute, and then stabilize. Once stabilized, cancel the `simulateStreamingInput` script; the reported numbers should now start to decrease.\n",
    "3. You can terminate the Spark Streaming and `simulateStreamingInput` jobs by interrupting Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/11/18 15:24:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleting existing files in stream-IN ...\n",
      "... done\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/11/18 15:24:40 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/11/18 15:24:40 INFO SparkContext: Submitted application: 3-salesPerMinutes.py\n",
      "20/11/18 15:24:40 INFO SecurityManager: Changing view acls to: bigdata\n",
      "20/11/18 15:24:40 INFO SecurityManager: Changing modify acls to: bigdata\n",
      "20/11/18 15:24:40 INFO SecurityManager: Changing view acls groups to: \n",
      "20/11/18 15:24:40 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/11/18 15:24:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bigdata); groups with view permissions: Set(); users  with modify permissions: Set(bigdata); groups with modify permissions: Set()\n",
      "20/11/18 15:24:41 INFO Utils: Successfully started service 'sparkDriver' on port 43343.\n",
      "20/11/18 15:24:41 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/11/18 15:24:41 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/11/18 15:24:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/11/18 15:24:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/11/18 15:24:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f77cad69-a63a-4d62-906d-a9eb72530e58\n",
      "20/11/18 15:24:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/11/18 15:24:41 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/11/18 15:24:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/11/18 15:24:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://bigdata-vm:4040\n",
      "20/11/18 15:24:42 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/11/18 15:24:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45317.\n",
      "20/11/18 15:24:42 INFO NettyBlockTransferService: Server created on bigdata-vm:45317\n",
      "20/11/18 15:24:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/11/18 15:24:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bigdata-vm, 45317, None)\n",
      "20/11/18 15:24:42 INFO BlockManagerMasterEndpoint: Registering block manager bigdata-vm:45317 with 366.3 MB RAM, BlockManagerId(driver, bigdata-vm, 45317, None)\n",
      "20/11/18 15:24:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bigdata-vm, 45317, None)\n",
      "20/11/18 15:24:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bigdata-vm, 45317, None)\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:24:55\n",
      "-------------------------------------------\n",
      "2000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:25:10\n",
      "-------------------------------------------\n",
      "2000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:25:25\n",
      "-------------------------------------------\n",
      "2000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:25:40\n",
      "-------------------------------------------\n",
      "2000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:25:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:26:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:26:25\n",
      "-------------------------------------------\n",
      "4000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:26:40\n",
      "-------------------------------------------\n",
      "9000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:26:55\n",
      "-------------------------------------------\n",
      "14000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:27:10\n",
      "-------------------------------------------\n",
      "19000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:27:25\n",
      "-------------------------------------------\n",
      "20000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:27:40\n",
      "-------------------------------------------\n",
      "20000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 15:27:55\n",
      "-------------------------------------------\n",
      "18000\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdata/Desktop/exercises_tests/3 - Kafka and Streaming/Spark Streaming/3-salesPerMinutes.py\", line 55, in <module>\n",
      "    ssc.awaitTermination()\n",
      "  File \"/home/bigdata/spark/python/lib/pyspark.zip/pyspark/streaming/context.py\", line 192, in awaitTermination\n",
      "  File \"/home/bigdata/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
      "  File \"/home/bigdata/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "  File \"/home/bigdata/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n",
      "  File \"/home/bigdata/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/bigdata/spark/python/lib/pyspark.zip/pyspark/context.py\", line 270, in signal_handler\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master \"local[*]\" 3-salesPerMinutes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3** Copy `3-salesPerMinute.py` into a file `3.3-top5Securities.py` and \n",
    "modify the latter to compute the top five most traded securities in the last 3minutes, which is updated every 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file 3.3-top5Securities.py\n",
    "# Your solution here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --master \"local[*]\" 3.3-top5Securities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A SparkStreaming Example that receives input from Kafka and outputs to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been using Spark Streaming to read from the filesystem, and output to the console or the filesystem. In this final exercise, we will run a spark streaming job that consumes input from Kafka. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first Create a new Kafka topic that will be used to receives stock quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n",
      "Created topic bigdata.orders.\n"
     ]
    }
   ],
   "source": [
    "# Create a new Kafka topic that will be used to receive stock quotes\n",
    "!kafka-topics.sh  --create --zookeeper localhost:2181 \\\n",
    "    --topic $USER.orders --partitions 5 --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the script that we will use to analyze the kafka topic. Check that you understand what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 4-salesPerMinuteFromKafka.py\n"
     ]
    }
   ],
   "source": [
    "%%file 4-salesPerMinuteFromKafka.py\n",
    "import sys\n",
    "import os\n",
    "import pwd\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"ERROR\")   #Make sure warnings and errors observed by spark are printed.\n",
    "\n",
    "ssc = StreamingContext(sc, 5)  #generate a mini-batch every 5 seconds\n",
    "zookeeper = \"localhost:2181\"\n",
    "username = pwd.getpwuid( os.getuid() )[ 0 ] \n",
    "topic = username + \".orders\"\n",
    "inputStream = KafkaUtils.createStream(ssc, zookeeper,\n",
    "                                  \"raw-event-streaming-consumer\", {topic:1})\n",
    "\n",
    "def parseOrder(line):\n",
    "  '''parses a single line in the orders file'''\n",
    "  s = line.strip().split(\",\")\n",
    "  try:\n",
    "      if s[6] != u\"B\" and s[6] != u\"S\":\n",
    "        raise Exception('Wrong format ' + str(s))\n",
    "      return [{\"time\": datetime.strptime(s[0], \"%Y-%m-%d %H:%M:%S\"),\n",
    "               \"orderId\": int(s[1]), \n",
    "               \"clientId\": int(s[2]),\n",
    "               \"symbol\": s[3], \n",
    "               \"amount\": int(s[4]), \n",
    "               \"price\":  float(s[5]), \n",
    "               \"buy\": s[6] == u\"B\"}]\n",
    "  except Exception as err:\n",
    "      print(\"Wrong line format (%s): %s\" % (line,err))\n",
    "      return []\n",
    "\n",
    "from operator import add\n",
    "orders = inputStream.map(lambda x: x[1]).flatMap(parseOrder)\n",
    "ordersPerMinute = orders.map(lambda o: 1).window(60, 15) # windows lenth = 60 sec, slide = 15 sec\n",
    "orderCountPerMinute = ordersPerMinute.reduce(add)\n",
    "orderCountPerMinute.pprint()\n",
    "\n",
    "# windows operations requires checkpointing\n",
    "sc.setCheckpointDir(\"checkpoint\")\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script `scripts/streamOrdersToKafka.py` can be used send the contents of `data/orders.txt` to the `$USER.orders` kafka topics, line by line, with 1 line published every 0.5 seconds. Execute this script in a separate notebook.\n",
    "\n",
    "In parallel, execute the following cell to run the spark streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify `--packages org.apache....` so that the driver required to connect to kafka from python\n",
    "# is automatically downloaded if we don't already have it.\n",
    "!spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.4 4-salesPerMinuteFromKafka.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
